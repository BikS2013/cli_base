# Universal CLI Design Specification

A comprehensive command line interface template that provides standardized support for commands, subcommands, complex configurations, profiles management, and multi-level configuration files.

## Overview

This CLI tool framework is designed to:
- Support commands, subcommands, options/parameters, and flags
- Manage complex dependencies through named profiles
- Handle configuration at global, local, and named file levels
- Provide flexible configuration file management
- Present information with colorful, readable output
- Offer comprehensive help and schema documentation
- Support LLM integration through LangChain

## Command Structure

```
cli-tool [COMMAND] [SUBCOMMAND] [OPTIONS] [FLAGS]
```

## Core Commands

The tool supports the following primary commands:

- `config`: Manage configuration files (global, local, named)
- `llm`: Manage profiles for LLM providers and models
- `ask`: Ask a question to an LLM
- `chat`: Start an interactive chat session with an LLM
- `schema`: Display command structure information
- `help`: Display help information

## Profile Management

Profiles provide a way to manage complex dependencies with multiple parameters. The general syntax for profile management is:

```
cli-tool [TYPE] [ACTION] [OPTIONS] [CONFIG-SCOPE]
```

Where:
- `[TYPE]`: The type of profile (e.g., `llm`)
- `[ACTION]`: The action to perform (`create`, `list`, `edit`, `delete`, `use`, `show`)
- `[OPTIONS]`: Profile-specific options and parameters
- `[CONFIG-SCOPE]`: Where to apply the action (`--global`, `--local`, `--file <filepath>`)

### LLM Profile Example

```
cli-tool llm create --name "gpt4-standard" --provider "openai" --model "gpt-4" --api-key "key123" --temperature 0.7 --global
cli-tool llm list --local
cli-tool llm use --name "gpt4-standard"
cli-tool llm show --name "claude-opus" --file "~/my-configs/ai-tools.json"
```

JSON-based creation and editing is also supported:

```
cli-tool llm create '{"name": "gpt4-standard", "provider": "openai", "model": "gpt-4", "api_key": "key123", "temperature": 0.7}' --global
cli-tool llm edit --name "gpt4-standard" '{"temperature": 0.5}' --global
```

## LLM Usage

The CLI provides commands for LLM interaction:

```
cli-tool ask "Your prompt here" --profile "gpt4-standard"
cli-tool chat --profile "claude-opus"
```

These commands support options like:
- `--profile, -p`: Specify which LLM profile to use
- `--stream/--no-stream`: Enable/disable streaming responses (for `ask` command)
- `--max-tokens`: Override max tokens for this request (for `ask` command)
- `--temperature`: Override temperature for this request (for `ask` command)

## Configuration Management

The tool supports three levels of configuration files:

1. **Global**: System-wide settings stored in `~/.cli-tool/config.json`
2. **Local**: Project-specific settings stored in `./.cli-tool/config.json`
3. **Named**: User-specified configuration files at any location

Configuration management commands allow working with these files:

```
cli-tool config [ACTION] [CONFIG-SCOPE] [OPTIONS]
```

### Configuration Actions

- `show`: Display configuration content
- `save`: Save current parameters to configuration
- `update`: Update configuration with current parameters
- `replace`: Replace entire configuration with current parameters
- `import`: Import configuration from another file
- `export`: Export configuration to another file
- `reset`: Reset configuration to defaults
- `generate`: Generate command-line instructions based on configuration
- `status`: Display complete runtime settings status

### Config Examples

```
cli-tool config show --global
cli-tool config save --local
cli-tool config update --file "~/my-configs/special.json"
cli-tool config import --from-file "~/my-configs/special.json" --to-global
cli-tool config export --from-local --to-file "~/my-configs/special.json"
cli-tool config generate --local
cli-tool config status --include-paths --no-configs --format json
```

## Configuration Hierarchy

When resolving parameters, the tool uses the following precedence (highest to lowest):

1. Command line arguments
2. Named configuration (if specified with `--file`)
3. Local configuration
4. Global configuration
5. Default values

## LLM Profile Structure

LLM profiles use the following JSON structure:

```json
{
  "name": "profile-name",
  "provider": "openai",
  "model": "gpt-4",
  "api_key": "YOUR_API_KEY",
  "temperature": 0.7,
  "max_tokens": 2048,
  "base_url": "https://api.openai.com/v1",
  "api_version": "v1",
  "deployment": "deployment-name",
  "organization": "org-id",
  "region": "us-west-2",
  "project_id": "your-google-project",
  "timeout": 30
}
```

The CLI supports multiple LLM providers including:
- OpenAI
- Anthropic
- Google
- Azure OpenAI
- AWS Bedrock
- Ollama
- LiteLLM
- Cohere
- Mistral
- Together AI

## Complete Configuration File Structure

The configuration files use the following JSON structure:

```json
{
  "profiles": {
    "llm": {
      "profile-name-1": {
        "name": "profile-name-1",
        "provider": "openai",
        "model": "gpt-4",
        "api_key": "YOUR_API_KEY",
        "temperature": 0.7,
        "max_tokens": 2048
      },
      "profile-name-2": {
        "name": "profile-name-2",
        "provider": "anthropic",
        "model": "claude-3-opus",
        "api_key": "YOUR_API_KEY",
        "temperature": 0.5,
        "max_tokens": 4096
      }
    }
  },
  "defaults": {
    "llm": "profile-name-1"
  },
  "settings": {
    "output_format": "json",
    "color_theme": "dark",
    "log_level": "info"
  },
  "commands": {
    "ask": {
      "stream": true,
      "temperature": 0.7
    }
  }
}
```

## Help Documentation

The tool provides comprehensive help information:

```
cli-tool help
cli-tool help config
cli-tool help llm
cli-tool help llm create
```

## Schema Visualization

The schema command presents information about commands, options, and features:

```
cli-tool schema
cli-tool schema export
cli-tool schema show
cli-tool schema show config
```

## Advanced Features

### Runtime Settings JSON Representation

The tool can provide a comprehensive view of its runtime state:

```
cli-tool config status
cli-tool config status --include-paths --no-configs --format json
```

This allows for inspection of:
- Current scope
- Loaded configuration files
- Effective configuration settings
- Command line arguments
- Available profiles

### Verbose Mode

All commands support verbose mode with the `-v` or `--verbose` flag:

```
cli-tool -v config show
cli-tool --verbose llm list
```

In verbose mode, the tool provides additional information about:
- Runtime settings and paths
- Parameter resolution
- Command execution details

## Usage Examples

### Creating and Using LLM Profiles

```bash
# Create a new LLM profile in the global configuration
cli-tool llm create --name "gpt4-standard" --provider "openai" --model "gpt-4" --api-key "key123" --temperature 0.7 --global

# Create a profile with JSON input
cli-tool llm create '{"name": "claude-opus", "provider": "anthropic", "model": "claude-3-opus", "api_key": "key456", "temperature": 0.5}' --local

# List all globally available LLM profiles
cli-tool llm list --global

# Use a specific LLM profile as the default
cli-tool llm use --name "gpt4-standard"

# Show details of a specific profile
cli-tool llm show --name "claude-opus"

# Edit a profile parameter
cli-tool llm edit --name "gpt4-standard" --temperature 0.5 --global

# Delete a profile
cli-tool llm delete --name "old-profile" --global
```

### Working with Configuration Files

```bash
# Show content of the local configuration file
cli-tool config show --local

# Show content of a named configuration file
cli-tool config show --file "~/my-configs/special.json"

# Save current parameters to global configuration
cli-tool config save --global

# Update global configuration with local values
cli-tool config import --from-local --to-global

# Update local configuration with global values
cli-tool config import --from-global --to-local

# Replace local configuration with a named file
cli-tool config import --from-file "~/my-configs/special.json" --to-local --replace

# Generate command-line instructions from configuration
cli-tool config generate --file "~/my-configs/special.json"

# Display runtime settings status
cli-tool config status --format json
```

### Using LLMs

```bash
# Ask a question to an LLM
cli-tool ask "Write a limerick about programming" --profile "gpt4-standard"

# Ask with specific parameters
cli-tool ask "Explain quantum physics" --temperature 0.2 --max-tokens 500

# Start an interactive chat session
cli-tool chat --profile "claude-opus"
```

### Using Help and Schema

```bash
# Show general help
cli-tool help

# Show help for a specific command
cli-tool help llm create

# Show complete command schema
cli-tool schema

# Show detailed schema for a specific command
cli-tool schema show llm
```

## Implementation Architecture

The CLI is built with a modular architecture:

1. **Command Registry**: Central system that tracks and organizes all commands
2. **Parameter Resolution**: System for resolving parameters from multiple sources
3. **Runtime Settings**: Class that manages configuration and runtime state
4. **Profile Management**: Base classes for creating and managing profiles
5. **Extensibility**: Generators for creating new commands from templates
6. **Context Management**: Singleton pattern for sharing state across components
7. **Output Formatting**: Consistent formatting of command output

These components work together to provide a powerful, flexible, and extensible CLI framework that can be used as a foundation for building advanced command-line tools.